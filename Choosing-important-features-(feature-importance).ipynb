{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "Choosing important features (feature importance)\n",
    "--------------------------------------------------\n",
    "\n",
    "Feature importance is the technique used to select features using a trained supervised classifier. When we train a classifier such as a decision tree, we evaluate each attribute to create splits; we can use this measure as a feature selector. Let’s understand it in detail.\n",
    "\n",
    "Random forests are among the most popular machine learning methods thanks to their relatively good accuracy, robustness, and ease of use. They also provide two straightforward methods for feature selection—mean decrease impurity and mean decrease accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otto Train data\n",
    "\n",
    "You can download training dataset, train.csv.zip, from the https://www.kaggle.com/c/otto-group-product-classification-challenge/data and place the unzipped train.csv file in your working directory.\n",
    "\n",
    "This dataset describes 93 obfuscated details of more than 61,000 products grouped into 10 product categories (for example, fashion, electronics, and so on). Input attributes are the counts of different events of some kind.\n",
    "\n",
    "The goal is to make predictions for new products as an array of probabilities for each of the 10 categories, and models are evaluated using multiclass logarithmic loss (also called cross entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create Train and Test set from the original dataset \n",
    "def getTrainTestData(dataset,split):\n",
    "    np.random.seed(0) \n",
    "    training = [] \n",
    "    testing = []\n",
    "    np.random.shuffle(dataset) \n",
    "    shape = np.shape(dataset)\n",
    "    trainlength = np.uint16(np.floor(split*shape[0]))\n",
    "    for i in range(trainlength): \n",
    "        training.append(dataset[i])\n",
    "    for i in range(trainlength,shape[0]): \n",
    "        testing.append(dataset[i])\n",
    "    training = np.array(training) \n",
    "    testing = np.array(testing)\n",
    "    return training,testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate model performance\n",
    "def getAccuracy(pre,ytest): \n",
    "    count = 0\n",
    "    for i in range(len(ytest)):\n",
    "        if ytest[i]==pre[i]: \n",
    "            count+=1\n",
    "    acc = float(count)/len(ytest)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset  (35000, 94)\n",
      "Size of Data set before feature selection: 26.32 MB\n"
     ]
    }
   ],
   "source": [
    "#Load dataset as pandas data frame\n",
    "data = read_csv('data/ottoTrain.csv')\n",
    "#Extract attribute names from the data frame\n",
    "feat = data.keys()\n",
    "feat_labels = feat.get_values()\n",
    "#Extract data values from the data frame\n",
    "dataset = data.values\n",
    "#Shuffle the dataset\n",
    "np.random.shuffle(dataset)\n",
    "#We will select 50000 instances to train the classifier\n",
    "inst = 50000\n",
    "#Extract 50000 instances from the dataset\n",
    "dataset = dataset[0:inst,:]\n",
    "#Create Training and Testing data for performance evaluation\n",
    "train,test = getTrainTestData(dataset, 0.7)\n",
    "#Split data into input and output variable with selected features\n",
    "Xtrain = train[:,0:94] \n",
    "ytrain = train[:,94] \n",
    "shape = np.shape(Xtrain)\n",
    "print(\"Shape of the dataset \",shape)\n",
    "#Print the size of Data in MBs\n",
    "print(\"Size of Data set before feature selection:\",(Xtrain.nbytes/1e6),\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for building the Tree is: 11.645748\n",
      "Accuracy of model before feature selection is 98.82\n"
     ]
    }
   ],
   "source": [
    "#Lets select the test data for model evaluation purpose\n",
    "Xtest = test[:,0:94] \n",
    "ytest = test[:,94]\n",
    "#Create a random forest classifier with the following Parameters\n",
    "trees= 250\n",
    "max_feat= 7\n",
    "max_depth = 30\n",
    "min_sample = 2\n",
    "clf = RandomForestClassifier(n_estimators=trees, max_features=max_feat, max_depth=max_depth, \n",
    "min_samples_split= min_sample, random_state=0,n_jobs=-1)\n",
    "#Train the classifier and calculate the training time\n",
    "import time\n",
    "start = time.time() \n",
    "clf.fit(Xtrain, ytrain) \n",
    "end = time.time()\n",
    "#Lets Note down the model training time\n",
    "print(\"Execution time for building the Tree is: %f\"%(float(end)- float(start)))\n",
    "pre = clf.predict(Xtest)\n",
    "#Let's see how much time is required to train the model on the training dataset:\n",
    "#Evaluate the model performance for the test data\n",
    "acc = getAccuracy(pre, ytest)\n",
    "print(\"Accuracy of model before feature selection is\",(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 0.3334665042017519)\n",
      "('feat_1', 0.003618695862880122)\n",
      "('feat_2', 0.003724305088853096)\n",
      "('feat_3', 0.01157921747206275)\n",
      "('feat_4', 0.010297382675187447)\n",
      "('feat_5', 0.0010359139416194119)\n",
      "('feat_6', 0.0003817133603805617)\n",
      "('feat_7', 0.0024867672489765026)\n",
      "('feat_8', 0.00966897216105461)\n",
      "('feat_9', 0.007906150362995095)\n",
      "('feat_10', 0.002234248080213037)\n",
      "('feat_11', 0.03032120226642743)\n",
      "('feat_12', 0.0011208629500706663)\n",
      "('feat_13', 0.003991984466073026)\n",
      "('feat_14', 0.0194087068806635)\n",
      "('feat_15', 0.01539863449663281)\n",
      "('feat_16', 0.0055203970543115455)\n",
      "('feat_17', 0.007198233904267588)\n",
      "('feat_18', 0.0036309310056707516)\n",
      "('feat_19', 0.003800885800560713)\n",
      "('feat_20', 0.004600100163709177)\n",
      "('feat_21', 0.0012839572570891805)\n",
      "('feat_22', 0.0034580481856073624)\n",
      "('feat_23', 0.001941425686466054)\n",
      "('feat_24', 0.009502403878816025)\n",
      "('feat_25', 0.01838207049845683)\n",
      "('feat_26', 0.022011162365845237)\n",
      "('feat_27', 0.008292147847657359)\n",
      "('feat_28', 0.003155738407834562)\n",
      "('feat_29', 0.0024792257598606755)\n",
      "('feat_30', 0.006647623919309846)\n",
      "('feat_31', 0.001259992364310767)\n",
      "('feat_32', 0.008187326942297636)\n",
      "('feat_33', 0.005608890706633676)\n",
      "('feat_34', 0.036628469546452595)\n",
      "('feat_35', 0.006349884154746056)\n",
      "('feat_36', 0.01385145012418643)\n",
      "('feat_37', 0.0033450862421822328)\n",
      "('feat_38', 0.004866248685308569)\n",
      "('feat_39', 0.00835829252405806)\n",
      "('feat_40', 0.019564634122549947)\n",
      "('feat_41', 0.00373852075301625)\n",
      "('feat_42', 0.016443622996113044)\n",
      "('feat_43', 0.006583342767740538)\n",
      "('feat_44', 0.002971017509732847)\n",
      "('feat_45', 0.003141237240091426)\n",
      "('feat_46', 0.0074046769199337305)\n",
      "('feat_47', 0.005484716424559312)\n",
      "('feat_48', 0.007917434311117483)\n",
      "('feat_49', 0.0024748089590100704)\n",
      "('feat_50', 0.0055491610952644165)\n",
      "('feat_51', 0.0007780710153536747)\n",
      "('feat_52', 0.0013770978966777155)\n",
      "('feat_53', 0.006283820176418601)\n",
      "('feat_54', 0.007040913366014783)\n",
      "('feat_55', 0.002763734701696224)\n",
      "('feat_56', 0.0033675633546134554)\n",
      "('feat_57', 0.01003413203368018)\n",
      "('feat_58', 0.004836650907569536)\n",
      "('feat_59', 0.006539834280515092)\n",
      "('feat_60', 0.0292327078209446)\n",
      "('feat_61', 0.008227619278327289)\n",
      "('feat_62', 0.012654119449234742)\n",
      "('feat_63', 0.0013136663846220632)\n",
      "('feat_64', 0.007724804771344403)\n",
      "('feat_65', 0.001612431520676713)\n",
      "('feat_66', 0.004058900327604486)\n",
      "('feat_67', 0.014128729007342088)\n",
      "('feat_68', 0.007844172422861452)\n",
      "('feat_69', 0.01111308031993872)\n",
      "('feat_70', 0.004887911520735957)\n",
      "('feat_71', 0.0049887388367168065)\n",
      "('feat_72', 0.007651406618761344)\n",
      "('feat_73', 0.002927446705219201)\n",
      "('feat_74', 0.002325939616804964)\n",
      "('feat_75', 0.011811733886280304)\n",
      "('feat_76', 0.008350364992196414)\n",
      "('feat_77', 0.0016130280209724023)\n",
      "('feat_78', 0.007477042932822354)\n",
      "('feat_79', 0.0038636267679849658)\n",
      "('feat_80', 0.007134051373971065)\n",
      "('feat_81', 0.0012958121098888358)\n",
      "('feat_82', 0.0026819501244384806)\n",
      "('feat_83', 0.00558821308792052)\n",
      "('feat_84', 0.000866685759505325)\n",
      "('feat_85', 0.005252319291513066)\n",
      "('feat_86', 0.013716186370718414)\n",
      "('feat_87', 0.003489217339235223)\n",
      "('feat_88', 0.008307759484480873)\n",
      "('feat_89', 0.0024315803794271946)\n",
      "('feat_90', 0.014139709447807009)\n",
      "('feat_91', 0.004049053368741259)\n",
      "('feat_92', 0.004735064841981027)\n",
      "('feat_93', 0.001208680445831377)\n"
     ]
    }
   ],
   "source": [
    "#Once we have trained the model we will rank all the features \n",
    "for feature in zip(feat_labels, clf.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                 class_weight=None,\n",
       "                                                 criterion='gini', max_depth=30,\n",
       "                                                 max_features=7,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=250, n_jobs=-1,\n",
       "                                                 oob_score=False,\n",
       "                                                 random_state=0, verbose=0,\n",
       "                                                 warm_start=False),\n",
       "                max_features=None, norm_order=1, prefit=False, threshold=0.01)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select features which have higher contribution in the final prediction\n",
    "sfm = SelectFromModel(clf, threshold=0.01) \n",
    "sfm.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data set before feature selection:  5.6  MB\n",
      "Shape of the dataset  (35000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Transform input dataset\n",
    "Xtrain_1 = sfm.transform(Xtrain) \n",
    "Xtest_1= sfm.transform(Xtest)\n",
    "#Let's see the size and shape of new dataset \n",
    "print(\"Size of Data set before feature selection: \",(Xtrain_1.nbytes/1e6),\" MB\")\n",
    "shape = np.shape(Xtrain_1)\n",
    "print(\"Shape of the dataset \",shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(15000, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_1.shape\n",
    "Xtest_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for building the Random Forest is:  5.580687522888184\n",
      "Accuracy after feature selection  99.97333333333333\n"
     ]
    }
   ],
   "source": [
    "#Model training time\n",
    "start = time.time() \n",
    "clf.fit(Xtrain_1, ytrain) \n",
    "end = time.time()\n",
    "print(\"Execution time for building the Random Forest is: \",(float(end)- float(start)))\n",
    "#Let's evaluate the model on test data\n",
    "pre = clf.predict(Xtest_1) \n",
    "count = 0\n",
    "acc2 = getAccuracy(pre, ytest)\n",
    "print(\"Accuracy after feature selection \",(100*acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
